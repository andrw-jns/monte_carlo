---
title: |
  |    Bootstrapping and Monte Carlo
  |    to Aid Parameter Setting
author: ''
date: "14 September 2017"
output:
  pdf_document: 
    fig_caption: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This paper highlights two statistical techniques which may allow predictions from individual workshop participants to be turned into a single model parameter. 

Workshop participants are typically asked to forecast reduction in activity for a given point in the future (compared to current levels). A single value for a given metric may be set by asking participants to reach a consensus in the workshop. An alternative method involves collecting individuals estimates and using statistical methods to produce a parameter.

Two potential scenarios for the "statistical methods" approach are considered in this report:

1. Workshop participants are asked to provide a point estimate for a metric. 

1. Workshop participants are asked to provide a prediction interval for a metric.

The first scenario would appear easy to implement in the workshop, with the data processing being equally straightforward. The second would likely require some extra explanation, possibly based on _How to measure anything_ (Hubbard, 2014). It would also involve assumptions detailed in Section 2.  __What are the benefits?__

## 1. Point Estimates: The Bootstrap Method

In the following example, fifteen participants provide a *point estimate* for a given metric. It is important to note that a distribution of estimates may take any form, and that the form may also vary from metric to metric. In this example, participants' estimates are normally distributed around a mean value of 0.95 (a 5% reduction), with a standard deviation of 0.02:  
  

```{r, include=FALSE}
library(tidyverse)
suppressPackageStartupMessages(library(tidyverse))
```

```{r, echo=TRUE}

set.seed(102)

(point_estimates <- round(rnorm(15, 0.95, 0.02),2))

```

```{r, echo=FALSE, out.width= "50%", fig.align='center'}
tmp_df <- tibble(x = "", y = point_estimates)

ggplot(tmp_df, aes(x, y))+
  stat_boxplot(geom ='errorbar') + 
  geom_boxplot()+
   geom_jitter(size = 3)+
  scale_x_discrete(expand = c(1,1))+
  ylab("Value")+
  labs(caption = "Fig. 1. Distribution of point estimates")+
  theme(plot.caption = element_text(hjust = 0)) 
```

The aim is to aggregate these 15 points into a single estimate, and provide a measurement of uncertainty. This information will then be translated into a model parameter.


At this point, we shall consider the bootstrap method (as an alternative to simple descriptive statistics). Bootstrapping involves randomly sampling values (with replacement) from the original data to produce a large number of virtual samples. Each of these samples is set to be the same size as the original. 

This method has 3 potential benefits:

1. Since we cannot assume our estimates will be normally distributed, the bootstrap provides a statistically sound measure of uncertainty around almost any sample statistic, regardless of the distribution.

1. It replicates the effect of taking a large number of samples from the population (in this case, subject experts).

1. There is evidence to suggest that the bootstrap method can produce outcomes very similar to a consensus view [^Delphi_paper]. 

[^Delphi_paper]:  _Stability of response characteristics of a Delphi panel: application of bootstrap data expansion_ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1318466/

A bootstrapping method may follow the steps below[^dummies]:

[^dummies]: _the-bootstrap-method-for-standard-errors-and-confidence-intervals_ :  http://www.dummies.com/education/science/biology/the-bootstrap-method-for-standard-errors-and-confidence-intervals/

```{r}
# Find the mean and median of the set of points estimates:
(points_mean   <- mean(point_estimates))
(points_median <- median(point_estimates))
```


```{r}
# Create a total of 100k samples:

for(i in 1:99999){
  tmp <- sample(point_estimates, length(point_estimates), replace = T)
  
  points_mean[1 + i]   <- mean(tmp)
  points_median[1 + i] <- median(tmp)
  rm(tmp)
}


# the standard error of the sample statistic is the standard deviation of the bootstrapped data:

sd(points_mean)

# compare with estimate of standard error from the original data:
sd(point_estimates) / sqrt(length(point_estimates))

# The bootstrap can be applied to a 
# wide range of sample statistics and distributions.

# We can obtain a standard error of the median:
sd(points_median)

# And confidence intervals for the mean and the median:
quantile(points_mean  , probs = c(0.025, 0.5, 0.975))
quantile(points_median, probs = c(0.025, 0.5, 0.975))


```
  
In summary we could for example produce the median, the standard error around the median, and resulting 95% confidence limits.

$0.96\pm0.01(0.95-0.97)$ 


## 2. Prediciton Intervals: Monte Carlo Simulation

The Monte Carlo method (or a similar method) is applicable if workshop participants are asked to provide a prediction interval.

For example, Participant A may be 90% confident that activity can be  reduced by 0 to 5% (0.95 to 1 times the current rate) by 2022 .

Unlike a point estimate, when given an interval we will have to make assumptions about the probability of obtaining values within the interval. It is likely that participants would - knowingly or unknowingly - assume a uniform distribution over the interval. 

A uniform distribution dictates that all values within the range are equally possible, yet an answer outside the interval is impossible. 

By contrast, a 90% confidence interval of a normal distribution would allow for the possibility of values outside the predicted range. However, participants would- knowingly or not - be stating that the most likely outcome lay at the centre of the interval.


### 2.1 Assuming a Normal Distribution

NB. The same method could be applied to uniform distributions.

Intervals will be collected from each participant. The boundaries of each interval will be the upper and lower (95%) confidence limits.

We can then deduce the mean, $\bar{x}$, which is the middle of the interval. The standard deviation, sd, for 95% confidence limits is found with  

$\frac{\pm CL-\bar{x}}{1.96}=\pm sd$ ,  

where $CL$ is the confidence limit.

A next step might be to amalgamate the individual distributions. However, rather than add distributions (which appears to involve non-trivial mathematics [^Wolfram] [^Wiki] it appeared reasonable to sample from each distribution in turn (one round of sampling) many times. This round-based method of sampling should produce an aggregate distribution (a numeric rather than analytical solution?), from which sample statistics could be calculated.


[^Wolfram]: http://mathworld.wolfram.com/NormalSumDistribution.html

[^Wiki]: https://en.wikipedia.org/wiki/Mixture_distribution

### Example

The following example, based on five participants, illustrates how the process might work. Participants provide interval limits. To reiterate, if we assume a normal distribution, a mean is found at the centre of each interval.

```{r , echo = FALSE}
set.seed(103)

intervals <- tibble(participant = LETTERS[1:5],
                    lcl         = seq(0, 0.04, by = 0.01),
                    ucl         = seq(0, 0.08, by = 0.02),
                    means       = ucl - (ucl-lcl)/2, # assume norm
                    sdevs       = (ucl-means)/qnorm(0.975)
                    ) 

```

Method 1 samples from each participant's distribution in turn (one round of sampling). This round of sampling is repeated thousands of times:   
   
     
     

```{r, out.width= "50%"}

meth_1 <- NULL

for(i in seq(1, 100000, by = 5)){
  meth_1[(i):(i+4)] <- pmap_dbl(list(n=1, intervals$means, intervals$sdevs), rnorm)
}

meth_1 <- tibble(estimates = meth_1)

ggplot(meth_1, aes(estimates))+
  geom_histogram(binwidth = 0.002)+
  labs(caption = "Fig. 2. Aggregate distribution using Method 1")+
  theme(plot.caption = element_text(hjust = 0)) 


quantile(meth_1$estimates, probs = c(0.025, 0.5, 0.975))

```

The code below verifies that an identical distribution is created by sampling once from any distribution, and repeating this thousands of times.  
  

```{r, out.width= "50%"}

meth_2 <- NULL

for(i in 1:50000){
  j <- sample(1:length(intervals$participant),1)
  meth_2[(i)] <- pmap_dbl(list(n=1,intervals$means[j], intervals$sdevs[j]), rnorm)
  rm(j)
}

meth_2 <- tibble(estimates = meth_2)

ggplot(meth_2, aes(estimates))+
  geom_histogram(binwidth = 0.002)+
  labs(caption = "Fig. 3. Aggregate distribution using Method 2 produces an identical result")+
  theme(plot.caption = element_text(hjust = 0)) 


quantile(meth_2$estimates, probs = c(0.025, 0.5, 0.975))

```


NB. With both these approaches, negative prediction values (growth in activity rates) would be possible.




